---
title: "Fundamentals of Missing Data in Evaluation"
subtitle: "Presentation to MSU Department of Psychology, Program Evaluation
           Occasional Speaker Series, East Lansing, MI"
author: 
  - name: Steven J. Pierce
    orcid: 0000-0002-0679-3019
    email: pierces1@msu.edu
institute: "Center for Statistical Training and Consulting"
date: December 5, 2024
title-slide-attributes: 
  data-notes: "1 hr (45-50 min talk + 10-15 min Q & A) My talk today focuses on 
  some concepts and issues related to missing data in program evaluation 
  contexts. My expertise lies in applied statistics, so I'll focus on aspects of 
  missing data relevant to quantitative methods and statistical analyses. Even 
  still, missing data is a big topic and this is just a short talk, so my goal 
  is to teach some fundamental ideas, share some insights and practical advice, 
  then offer resources you can use to learn more on your own. Jot down either my 
  email address or the link you see at the bottom of the screen. My slides, 
  complete with speaker notes, reference list, and links to resources, will be 
  available from that link."
csl: apa-numeric-superscript-brackets.csl
bibliography: references.bib
format: 
  revealjs:
    embed-resources: true
    theme: [default, CSTAT_theme.scss]
    controls: true
    slide-number: c/t
    show-notes: false
    header-logo: graphics/Combomark-Horiz_Green-RGB.png
    footer: <a href="https://github.com/sjpierce/FMDE2024">https://github.com/sjpierce/FMDE2024</a>
    logo: graphics/CSTAT_5x5_Transparent.png
    link-external-icon: true
    link-external-newwindow: true
filters:
  - reveal-header
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library(knitr)
library(kableExtra) # Functions for formatting nice tables. 
library(rgl)        # for cube3d()
```

## Outline
* What is missing data? 
* Why do we end up with missing data? 
* Why should we care about missing data? 
* How can we diagnose the missing data issues for a given study? 
* What should we do about missing data?
    * Prevention
    * Diagnosis
    * Treatment
* Advice

# What is missing data?
Missing data (MD) are measurements you want or intended to collect but did not 
get. [@Fernández-García-RN4151]

::: {.incremental}
* Having MD is *common* in research & evaluation studies.
* If you do much evaluation work, you *will* run into MD.
:::

::: {.notes}
In the simplest terms, missing data are simply measurements that you want or 
intended to collect but did not get. While you can sometimes categorize what
kind of missing data you have or why it is missing, the basic problem is that
you didn't get all the data you need. 

**>** Having missing data is a common issue afflicting research and evaluation 
studies. Entirely avoiding it is difficult and rare.

**>** If you do much evaluation work, you *will* eventually run into some 
missing data issues. Therefore, it is worth learning about the issue and how to
handle it.
:::

# Why do we end up with missing data? 
Data collection doesn't always go according to plan...
<br> <br> 

::: {.fragment}
```{r}
#| label: tbl-factors

data.frame(Human = c("Participant behavior", "Evaluator errors", 
                     "Partner behavior"),
           Other = c("Equipment failures", "Records/Databases", 
                     "Unusual Events")) %>%  
  kable(., format = "html", 
        col.names = c("Human Factors", "Other Factors")) %>% 
  kable_classic(full_width = FALSE, lightable_options = c("hover"), 
                html_font = "Lato")
```
:::

::: {.notes}
Like many things in life, data collection doesn't always go according to plan.
**>** Human factors often contribute to the occurrence of missing data. For 
example, a survey participant may skip a question without recording any
response, offer an illegible or irrelevant answer, refuse to answer it, or
simply say "I don't know". They may skip whole instruments or entire data
collection visits. But evaluators occasionally make errors that lead to missing
data. Imagine forgetting to take a data collection form with you, or programming
the skip pattern for an online survey incorrectly. Sometimes, our evaluation
partners' decisions or behavior can prevent obtaining some data we wanted.
Perhaps they object to sharing something with you, or were not diligent about
logging some events of interest.

Other things can go wrong too. Perhaps a physical instrument breaks, or its
battery dies, or your online survey app crashes. If you're using an
organization's records or databases, those may be incomplete, difficult to
search for relevant data, coded in unhelpful ways, or could have been lost or
damaged or even destroyed according to some retention policy. In terms of
events, consider the recent hurricanes in Florida that disrupted data collection
for one of my projects.
:::

# Missing Data & Project Lifecycle

```{dot}
//| fig-width: 9
//| fig-height: 6
digraph WhenMD {
rankdir="TB";

node [shape = "box", style= "filled", fontname="Lato", fillcolor = "#C3FFEC"]
Plan [label = "Study Planning & Design"]
Collect [label = "Data Collection"]
Enter [label = "Data Entry"]
Manage [label = "Data Storage & Management"]
Analyze [label = "Data Analysis"]
Plan -> Collect -> Enter -> Manage -> Analyze
}
```

::: {.notes}
Missing data result from events at various points in a project's lifecycle and 
that may affect your options to handle the problem. At the planning stage, one 
could leave out an item by accident while designing an instrument. Most missing 
data is probably a result of things happening during data collection events, 
but there are some post-data collection things that can happen too. Suppose you 
have staff entering data from paper forms into electronic files: simple data 
entry errors can introduce missing data at that stage. Similarly, at the data 
storage and management stage before analyses begin, someone might make a mistake 
while working with a data file or a computer hard drive might fail. Vigilance is
warranted throughout the project lifecycle. 
:::

# Why should we care about missing data? 

::: {.notes}
So, I've defined missing data and identified some of the ways we can end up with
missing data. But why should we care about it? That's a crucial question and 
I'll answer it. 
:::

## Ethics for Evaluators
Handling missing data well enacts our guiding principles [@AEA-RN8648]: 

:::: {.columns}

::: {.column width="25%"}
![](graphics/AEA_Logo.png){fig-alt="AEA logo." width=150% height=150%}
:::

::: {.column width="75%"}

::: {.incremental}
* Systematic inquiry
* Competence 
* Integrity 
:::

:::

::::

::: {.notes}
Perhaps the broadest answer to why we should care about missing data is that 
this is an ethics issue. Thoughtful handling of missing data is a good way to 
enact several of AEA's guiding principles for evaluators. 

* **>** *Systematic inquiry* demands that we examine the quality of the 
  available data and use it appropriately. We have an obligation to use sound 
  methodology. There are scientific consequences of missing data but there are 
  also legitimate, evidence-based methods for minimizing its impact on our 
  analyses, conclusions, and products. 
* **>** *Competence* demands that we invest in skills necessary to understand 
  and handle missing data properly because doing it poorly may compromise our 
  evaluations. Our funders, clients, and stakeholders--including the program 
  staff and the people and communities they serve and operate within--need and 
  deserve competent evaluation services. 
* **>** *Integrity* demands that we be transparent about the amount of missing 
  data, how we handled it, and how it may affect our evaluation results. 
:::

## Scientific Activities [@McKnight-RN1296]
There are 3 major scientific activities that can be affected by missing data.

::: {.incremental}
* Making structured observations of constructs.
* Using observations to draw inferences about relationships between constructs.
* Generalizing the results to populations beyond the collected sample.
::: 

::: {.notes}
Broadly speaking, scientific work consists of three major activities, each of
which has associated methodology concerns. 

* **>** First, we put effort into making structured observations of constructs. 
We have to be concerned about how missing data affects measurement of the 
constructs we want to study. 
* **>** Second, we use those observations to draw inferences about relationships 
between constructs. Here, we have to consider how missing data could affect the 
internal validity of our inferences. 
* **>** Finally, we often want to generalize our results to a broader population 
than the sample we collected. Missing data can affect the generalizability of 
our results. 

Let's explore each of those ideas in more detail.
:::

## Consequences for Measurement [@McKnight-RN1296]

* Availability of constructs 
* Decreased reliability due to increased error variance
* Bias from poor content coverage
* Construct validity

::: {.notes}
Missing data can affect the availability of a construct for your analyses. 
The extreme case would be a construct measured by a single variable.
If that's missing, then the entire construct is missing for that individual 
and it may be very difficult to recover that information. Missing data is still 
a problem when we use multiple indicators to measure a construct, though perhaps 
less severe, and it may depend on how you combine items to create the score. 

Missing data decreases the reliability and construct validity of our measures. 
Internal consistency reliability decreases because missing data increases the 
error variance in a measure. You may recall that reliability is a function of 
how many items are used, so fewer observed items leads to lower reliability. 

Further, some items may be more important indicators of a construct than others.
You may get a biased estimate of the construct due to poorer content coverage if 
they are missing. Construct validity depends on both reliability and content 
coverage, so missing data is a threat to validity.
:::

## Consequences for Internal Validity [@Fernández-García-RN4151; @McKnight-RN1296]

* Selection bias
* Compromised randomization
* Power and precision 
* Inaccurate model assumptions

::: {.notes}
Missing data is a threat to a study's internal validity for several reasons. 
Selection bias can result from systematic differences between subgroups of
participants on constructs relevant to the study conclusions. For example, those
subgroups could be treatment versus control groups, people who complete the
study versus drop out, or those who have complete data versus incomplete data.
These differences may manifest with respect to demographic variables,
covariates, focal predictors, mediators, moderators, or outcomes. Imagine
evaluating a longitudinal health study where people who start out with worst 
symptoms drop out before the posttest more often than those with mild symptoms. 
Dropping out leads to missing data at posttest. If we fail to handle that 
properly in the analysis, we may draw inaccurate conclusions about the 
longitudinal trend in outcome scores. Or perhaps the retention rates differ 
between treatment and control groups in that same study. Selection bias from 
such differences reduce the sample representativeness and become rival 
explanations for findings, making them a threat to internal validity. 

We use randomization to make groups equivalent on measured and unmeasured 
confounding variables as a way to strengthen validity. That works best in large 
samples. However, missing data can compromise that effect of randomization by 
reducing sample size. It can also reduce randomization's ability to achieve 
equivalence between groups on confounding variables when data are missing 
differentially across the groups. That reduces internal validity. 

Most statistical software defaults to listwise deletion of cases that have any
missing values on the variables involved in an analysis. Reducing the sample 
size will always reduce both power to detect effects and the precision of 
effect size estimates. In addition to reducing overall sample size, missing data 
may lead to more unbalanced sample sizes between groups in a study, which also 
reduces statistical power in many models. 

Finally, missing data can change the distributions of variables in ways that 
invalidate the assumptions of various statistical models. Some models are more 
robust than others, but making inaccurate modeling assumptions risks running 
analyses that will yield misleading conclusions. That's another threat to 
internal validity. 
:::

## Consequences for Generalizability [@Fernández-García-RN4151; @McKnight-RN1296]

A representative sample is crucial to generalizing to the intended population!

* Theory development & cumulative knowledge
* Policy & decision-making

::: {.notes}
We draw samples to study populations when it isn't feasible to collect the data
we want from the entire population. The key to getting inferences that properly
generalize from a sample to the intended population is getting a representative
sample. Biases creep into findings when we use non-representative samples.
Missing data can introduce systematic biases into our samples. Suppose you're
studying a behavior that's illegal or stigmatized in some way. Some people who
engage in it may be reluctant to provide all the data you want. If we ignore the
missing data problem, listwise deletion will result in analyzing the subset of
the intended population that's willing to answer our questions. That in effect
changes the population to which we could legitimately generalize our findings
from "people who engage in behavior x" to "people who engage in behavior x and
provided complete data". Many people fail to attend to that though and assume
their results still apply to the original intended population.

That has consequences for developing and testing scientific theories and 
accumulating knowledge across studies. An example is the effect of publication 
bias, also called the file drawer problem, on meta-analyses. We know that 
studies with non-significant findings are less likely to get published. That 
means the published literature has a missing data problem that can seriously 
distort the results of meta-analyses that aim to summarize results of a 
collection of published papers. 

Finally, the big picture here is that when missing data impair generalizability, 
we may be using inaccurate or misleading findings to inform public policy and 
important decisions.
:::

# How can we diagnose the missing data issues for a given study? 

::: {.notes}
So, I've laid some foundations by defining missing data and discussing why it 
matters, but each study is unique so the nature and extent of the missing data 
issues it faces will vary across studies. Now the question is "how can we 
diagnose the missing data issues for a given study?" because we can't make good 
choices about what to do about missing data problems until we thoroughly 
understand them. It's hard to solve a problem before you've adequately described 
it. I'm going to cover some concepts that pertain to describing the mechanisms, 
amounts, and patterns of missing data. 
:::

## Cattell's Data Box [@Cattell-RN8693] {.smaller}

:::: {.columns}

::: {.column width="50%"}

``` {r}
#| label: data-box
#| message: false
#| fig-width: 5
#| fig-height: 5

options(rgl.useNULL=TRUE)
open3d(silent = TRUE)
cube3d(scaleMatrix(5,5,5), col = "#C3FFEC", quads = 5) %>% 
  translate3d(1, 1, 1) %>% 
  shade3d()

axis3d(edge = 'x--', labels = c("1", "2", ".", ".", ".", "V"))
mtext3d(text = "Variable", edge = 'x++', line = 4, floating = TRUE)

axis3d(edge = 'y-+', labels = c("1", "2", ".", ".", ".", "T"))
mtext3d(text = "Time", edge = 'y++', at = 6, line = 5, floating = TRUE)

axis3d(edge = 'z--', labels = c("P", ".", ".", ".", "2", "1"))
mtext3d(text = "Person", edge = 'z++', line = 4, floating = TRUE)

view3d(theta = 0, phi = -60, zoom = .9, interactive = FALSE)
grid3d(side = c("x+", "y-", "z+"), col = "black")
rglwidget()
```

:::

::: {.column width="50%"}
<br> *How much data is there? Data volume is* $N_{values} = P \times V \times T$

* Slices through the cube represent subsets of data. 
* Constructs are often measured by groups of adjacent variables (items).  
* Missing data puts holes in your cube!

:::

::::

::: {.notes}
Raymond Cattell described data as forming a 3-dimensional box or cube, with the
dimensions being persons, variables, and occasions (or times). This is useful
for thinking about the total volume of data we aim to collect. At least for a
simple longitudinal study where we collect the same instruments for each person
at each time point, we can just multiply the number of desired participants by
the number of variables by the number of time points to get the number of
desired data values, which is the volume of the cube. Different slices through
the cube represent specific subsets of data. The top layer would be all the data
for a given person (all variables at all time points). The left-most vertical
slice would be all the data for a specific variable across all persons and time
points. The front face of the cube would be time 1 data on all variables for all
persons. We could further refine this idea by recognizing that constructs are
often measured by groups of adjacent variables (items).

Missing data puts holes in the cube, turning it into something like Swiss 
cheese. The impact of that depends on how much data volume is lost and any 
patterns in where and why those holes are located relative to the data you want 
to analyze. 
:::

## Types of Missingness

* Item level
* Construct level
* Person level 
* Person-period level (intermittent vs. dropout)

::: {.notes}
There are a variety of types of missingness to consider as we begin describing 
a given data set. One place to start involves examining different parts of the
data cube. For example, item-level missingness pertains to an individual 
variable. What percentage of its values are missing? Contrast that with 
construct-level missingness, which is when an entire construct is missing. Some 
scoring methods leave a construct missing if any of its items are missing, 
others may still yield a score despite some missing items. Construct missingness 
is usually a bigger problem than item-level missing data. 

Person-level missing data is when all the data for a person is missing. Refusing
to participate will usually cause person-level missing data. That's an extreme
example, but a related type of missingness happens at the person-period level
when a participant skips an entire data collection event. That affects all
constructs collected at that time point. In longitudinal studies, person-period 
missingness could be intermittent, with a participant skipping one event but 
returning for subsequent events, or it may be a result of dropout or attrition 
that affects all remaining time points.
:::

## Describing the Amount of MD [@McKnight-RN1296; @van-Buuren-RN3962]

Report numbers & percentages of: 

* Participants w/ *any data* at each time point (retention/attrition)
* Complete vs. incomplete cases (overall & by time point)
* Missing values for each variable & construct
* Reasons for attrition/dropout and other missing data

::: {.notes}
The dimensions of Cattell's data box suggests some obvious summaries of the 
amount of missing data. For example, we probably want to examine and report the 
number and percentage of participants in the study who have any data at each 
time point and use that to compute retention or attrition rates. Then we can go 
further and report the number of complete vs. incomplete cases, both overall and
separately by time point. Then, we can compute the numbers and percentages of 
missing values for each variable, both overall and separately by time point. 

Finally, it can be really useful to report on the reasons why participants drop 
out, skip data collection events, or have other missing data. 

All of these things get easier when you have a tracking system that records: 

* Whether & when participants attended data each collection event
* When & why participants exit the study 

:::

## Mechanisms of Missingness

* Missing completely at random (MCAR)
* Missing at random (MAR)
* Missing not at random (MNAR)

## MCAR
MCAR is when neither observed nor unobserved variables predict which data is 
missing. 

## MAR
MAR is when observed values predict which data is missing.

## MNAR
MNAR is when unobserved values predict which values are missing. 


# Diagnosis


## Predictors of Attrition & Missingness in Longitudinal Studies

* Study arm: Compare retention rates 
* Study site (in multisite studies)
* Baseline/pretest values of outcome variables may predict who drops out or has 
  missing values
* Other covariates (demographics, site, )

# Treatment

# Prevention

An ounce of prevention is better than a pound of cure

@de-Leeuw-RN8682, @Wisniewski-RN2978

# Advice

* Collaborate with a statistician! 

## Practical Options

* Item-level missingness in scale scores [@Graham-RN1615; @Newman-RN8223]


## References {.scrollable}

::: {#refs}
:::

